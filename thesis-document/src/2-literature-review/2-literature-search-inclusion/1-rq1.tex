\subsubsection{RQ 1}

\paragraph{Searches and initial screening}

\begin{table}[]
    \centering
    \caption{Criteria used in chosen search engines}
    \begin{tabular}{rllll}
        \toprule
        \multicolumn{1}{l}{\textbf{Search engine}} & \textbf{Search area}        & \textbf{Years} & \textbf{Subject area} & \textbf{Languages} \\
        \midrule
        Scopus                                     & titles, abstracts, keywords & 2000--2022     & computer science      & English            \\
        IEEE Xplore                                & all metadata                & 2000--2022     & ---                   & ---                \\
        ACM                                        & titles, abstracts           & 2000--2022     & ---                   & ---                \\
        SpringerLink                               & titles                      & 2000--2022     & ---                   & ---                \\
        Semantic Scholar                           & ---                         & 2000--2022     & computer science      & ---                \\
        DBLP                                       & ---                         & ---            & ---                   & ---                \\
        \bottomrule
        \label{tab:search-criteria-used}
    \end{tabular}
\end{table}

Exact search criteria used in each of the engines are shown in Table~\ref{tab:search-criteria-used}.
For most databases, the searches were executed using the previously formulated search string with adaptations particular to the database's search engine.
Two exceptions were Semantic Scholar and DBLP: in their cases, the search engine did not have enough support for complex combinations of terms.
To make up for this shortcoming, two searches queries were used for these engines: \texttt{user interface description language} and \texttt{abstract user interface description}.
Both of them match the original search query and should be viewed as a subset of all queries that could be created from the combinations of terms used.
Executing searches with more term combinations was deemed unpractical due to required effort and possible results.

To find as many relevant literature items as possible, engines' capabilities for narrowing the search area were used.
Scopus provided the most sensible option of searching within titles, abstracts and keywords out of the box.
IEEE provided a similar \emph{all metadata}\footnote{\url{https://ieeexplore.ieee.org/Xplorehelp/searching-ieee-xplore/command-search\#summary-of-data-fields}} option, including \enquote{abstract, title text, and indexing terms}.
ACM's search engine required manual crafting of a query searching within titles and abstracts;
there's no possibility of searching by index terms/keywords.
SpringerLink database only allows narrowing the search to the title area (it was necessary as full-text search returned tens of thousands of results.)
The rest of the engines did not allow any narrowing of search results.

\begin{table}[]
    \begin{threeparttable}[b]
        \centering
        \caption{Results of the first stage of the literature review for RQ 1}
        \begin{tabular}{@{}rrcc@{}}
            \toprule
            \multicolumn{1}{l}{\textbf{Search engine}}          & \multicolumn{1}{l}{\textbf{Search date}} & \multicolumn{1}{l}{\textbf{Search results}} & \multicolumn{1}{l}{\textbf{Papers qualified for further review}} \\
            \midrule
            Scopus                                              & 9.03.2022                                & 950                                         & 64                                                               \\
            IEEE                                                & 17.03.2022                               & 261                                         & 24                                                               \\
            ACM                                                 & 20.03.2022                               & 178                                         & 18                                                               \\
            SpringerLink                                        & 21.03.2022                               & 293                                         & 17                                                               \\
            \multirow{2}{*}{Semantic Scholar}                   & \multirow{2}{*}{26.03.2022}              & 36\ 600 (100)\tnote{1}                       & \multirow{2}{*}{28}                                              \\
                                                                &                                          & 17\ 600 (100)\tnote{2}                       &                                                                  \\
            \multirow{2}{*}{DBLP}                               & \multirow{2}{*}{30.03.2022}              & 23\tnote{1}                                 & \multirow{2}{*}{10}                                              \\
                                                                &                                          & 4\tnote{2}                                  &                                                                  \\
            \midrule
            \multicolumn{3}{r}{\textbf{Total}} & 119 \\
            \bottomrule
            \label{tab:results-first-stage-review-rq-1}
        \end{tabular}
        \begin{tablenotes}
            \item [1] Number of results for query \texttt{user interface description language}.
            \item [2] Number of results for query \texttt{abstract user interface description}.
        \end{tablenotes}
    \end{threeparttable}
\end{table}

The results of the search are presented in Table~\ref{tab:results-first-stage-review-rq-1}.
Excepting searches using Semantic Scholar, all results have been taken into consideration during the first screening.
As they returned too many results, only 10 pages of \emph{most relevant}\footnote{\url{https://www.semanticscholar.org/faq\#ranking-function}} documents (100 in total) were scanned for papers.

After concluding the searches, all the documents were compiled together in a single list.
Inclusion of papers referenced in the previously mentioned reviews, as well as exclusion of duplicate and unavailable papers resulted in a collection of 119 items eligible for the second screening for inclusion.

\paragraph{Second screening for inclusion}

\todo[inline]{take a look at this again!}
\begin{itemize}
    \item from 119 papers, 73 have been chosen for further consideration
    \item out of 46 papers not included:
    \begin{itemize}
        \item 5 were not papers (a poster, an introduction, a workshop or a special interest group description)
        \item 23 were deemed irrelevant to the question of description of user interfaces \textendash they were concerned e.g.\ with developing a design tool/environment, formulating a model-based design methodology/approach, or some other areas of model-based UI development not directly concerned with specifying interfaces
        \item 10 were deemed too broad in their scope (touched on interaction specification/design, extended an existing UIDL to fulfill a specific use case, or defined transformation methods)
        \item 8 were deemed not clear and specific enough in their descriptions and were therefore discarded
    \end{itemize}
\end{itemize}
