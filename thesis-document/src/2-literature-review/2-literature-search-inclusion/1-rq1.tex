\subsubsection{RQ 1}

\paragraph{Searches and initial screening}

\begin{table}[]
    \centering
    \caption{Criteria used in chosen search engines}
    \begin{tabular}{rllll}
        \toprule
        \multicolumn{1}{l}{\textbf{Search engine}} & \textbf{Search area}        & \textbf{Years} & \textbf{Subject area} & \textbf{Languages} \\
        \midrule
        Scopus                                     & titles, abstracts, keywords & 2000--2022     & computer science      & English            \\
        IEEE Xplore                                & all metadata                & 2000--2022     & ---                   & ---                \\
        ACM                                        & titles, abstracts           & 2000--2022     & ---                   & ---                \\
        SpringerLink                               & titles                      & 2000--2022     & ---                   & ---                \\
        Semantic Scholar                           & ---                         & 2000--2022     & computer science      & ---                \\
        DBLP                                       & ---                         & ---            & ---                   & ---                \\
        \bottomrule
        \label{tab:search-criteria-used}
    \end{tabular}
\end{table}

Exact search criteria used in each of the engines are shown in Table~\ref{tab:search-criteria-used}.
For most databases, the searches were executed using the previously formulated search string with adaptations particular to the database's search engine.
Two exceptions were Semantic Scholar and DBLP: in their cases, the search engine did not have enough support for complex combinations of terms.
To make up for this shortcoming, two searches queries were used for these engines: \texttt{user interface description language} and \texttt{abstract user interface description}.
Both of them match the original search query and should be viewed as a subset of all queries that could be created from the combinations of terms used.
Executing searches with more term combinations was deemed unpractical due to required effort and possible results.

To find as many relevant literature items as possible, engines' capabilities for narrowing the search area were used.
Scopus provided the most sensible option of searching within titles, abstracts and keywords out of the box.
IEEE provided a similar \emph{all metadata}\footnote{\url{https://ieeexplore.ieee.org/Xplorehelp/searching-ieee-xplore/command-search\#summary-of-data-fields}} option, including \enquote{abstract, title text, and indexing terms}.
ACM's search engine required manual crafting of a query searching within titles and abstracts;
there's no possibility of searching by index terms/keywords.
SpringerLink database only allows narrowing the search to the title area (it was necessary as full-text search returned tens of thousands of results.)
The rest of the engines did not allow any narrowing of search results.

\begin{table}[]
    \begin{threeparttable}[b]
        \centering
        \caption{Results of the first stage of the literature review for RQ 1}
        \begin{tabular}{@{}rrcc@{}}
            \toprule
            \multicolumn{1}{l}{\textbf{Search engine}}          & \multicolumn{1}{l}{\textbf{Search date}} & \multicolumn{1}{l}{\textbf{Search results}} & \multicolumn{1}{l}{\textbf{Papers qualified for further review}} \\
            \midrule
            Scopus                                              & 9.03.2022                                & 950                                         & 64                                                               \\
            IEEE                                                & 17.03.2022                               & 261                                         & 24                                                               \\
            ACM                                                 & 20.03.2022                               & 178                                         & 18                                                               \\
            SpringerLink                                        & 21.03.2022                               & 293                                         & 17                                                               \\
            \multirow{2}{*}{Semantic Scholar}                   & \multirow{2}{*}{26.03.2022}              & 36\ 600 (100)\tnote{1}                       & \multirow{2}{*}{28}                                              \\
                                                                &                                          & 17\ 600 (100)\tnote{2}                       &                                                                  \\
            \multirow{2}{*}{DBLP}                               & \multirow{2}{*}{30.03.2022}              & 23\tnote{1}                                 & \multirow{2}{*}{10}                                              \\
                                                                &                                          & 4\tnote{2}                                  &                                                                  \\
            \midrule
            \multicolumn{3}{r}{\textbf{Total}} & 119 \\
            \bottomrule
            \label{tab:results-first-stage-review-rq-1}
        \end{tabular}
        \begin{tablenotes}
            \item [1] Number of results for query \texttt{user interface description language}.
            \item [2] Number of results for query \texttt{abstract user interface description}.
        \end{tablenotes}
    \end{threeparttable}
\end{table}

The results of the search are presented in Table~\ref{tab:results-first-stage-review-rq-1}.
Excepting searches using Semantic Scholar, all results have been taken into consideration during the first screening.
As they returned too many results, only 10 pages of \emph{most relevant}\footnote{\url{https://www.semanticscholar.org/faq\#ranking-function}} documents (100 in total) were scanned for papers.

After concluding the searches, all the documents were compiled together in a single list.
Inclusion of papers referenced in the previously mentioned reviews, as well as exclusion of duplicate and unavailable papers resulted in a collection of 119 items eligible for the second screening for inclusion.

\paragraph{Second screening for inclusion}

During the second screening, the introduction and concluding sections were examined to determine each of the paper's fit for the review.
Out of 119 papers, 61 will be considered further;
58 were discarded due to various reasons:
\begin{itemize}
    \item 5 of them were \textbf{not actual papers} (but, e.g., a poster, an introduction, or a workshop description)
    \item 25 of them were considered \textbf{off-topic} in context of user interface descriptions \textendash\ they described, e.g., development of a design environment or formulation of a model-based development methodology
    \item 17 of them were decided to be on-topic, but \textbf{out of scope} of the research question \textendash\ they touched on interaction specification, extended an existing description language, or defined some model transformation methods
    \item additionally, 11 papers were deemed \textbf{not clear} and specific enough in their descriptions.
\end{itemize}

\paragraph{Final screening for inclusion}

In the final screening, papers were evaluated in terms of relevance in their entirety.
Out of 61 papers, 11 were chosen for the proper review.
The rest of them was rejected due to similar reasons as in the previous screening:
\begin{itemize}
    \item 12 were deemed \textbf{irrelevant} \textendash\ this time, the category also included, e.g., work describing an outdated language, comparison of UI description and implementation languages, or discussion of needs that UIDLs did not satisfy at the time
    \item 9 articles additionally used an existing UIDL in their work
    \item 11 were decided to be \textbf{out of scope}
    \item 18 were considered to be of \textbf{too low quality} to be useful
\end{itemize}

\paragraph{Backwards search and summary}

To supplement the search with papers that otherwise might not have been included, an informal search through references of the included papers has been carried out.
As a result, 4 additional papers will be included in the review, bringing the total to 15 pieces.


